{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "    \n",
    "import multiprocessing\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrpu88/jupyter_py3/lib/python3.6/site-packages/pysynphot/locations.py:46: UserWarning: PYSYN_CDBS is undefined; functionality will be SEVERELY crippled.\n",
      "  warnings.warn(\"PYSYN_CDBS is undefined; functionality will be SEVERELY \"\n",
      "/home/nrpu88/jupyter_py3/lib/python3.6/site-packages/pysynphot/locations.py:329: UserWarning: Extinction files not found in extinction\n",
      "  warnings.warn('Extinction files not found in %s' % (extdir, ))\n",
      "/home/nrpu88/jupyter_py3/lib/python3.6/site-packages/pysynphot/refs.py:118: UserWarning: No graph or component tables found; functionality will be SEVERELY crippled. No files found for ftp://ftp.stsci.edu/cdbs/mtab/*_tmg.fits\n",
      "  'functionality will be SEVERELY crippled. ' + str(e))\n",
      "/home/nrpu88/jupyter_py3/lib/python3.6/site-packages/pysynphot/refs.py:125: UserWarning: No thermal tables found, no thermal calculations can be performed. No files found for ftp://ftp.stsci.edu/cdbs/mtab/*_tmt.fits\n",
      "  'no thermal calculations can be performed. ' + str(e))\n"
     ]
    }
   ],
   "source": [
    "from starnet.utils.data_utils.augment import convolve_spectrum\n",
    "from starnet.utils.data_utils.restructure_spectrum import rebin, continuum_normalize, ensure_constant_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "import os\n",
    "\n",
    "\n",
    "# Define parameters needed for continuum fitting\n",
    "LINE_REGIONS = [[4210, 4240], [4250, 4410], [4333, 4388], [4845, 4886], [5160, 5200], [5874, 5916], [6530, 6590]]\n",
    "SEGMENTS_STEP = 10.  # divide the spectrum into segments of 10 Angstroms\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "home = os.getenv('HOME')\n",
    "scratch = os.getenv('SCRATCH')\n",
    "starnet_data_folder = os.path.join(home, 'StarNet/starnet/data/')\n",
    "#intrigoss_grid_path = os.path.join(home, 'projects/rrg-kyi/group_writable/spectra/grids/intrigoss/train/') \n",
    "#phoenix_grid_path = os.path.join(home, 'projects/rrg-kyi/group_writable/spectra/grids/phoenix.astro.physik.uni-goettingen.de/v2.0/HiResFITS/PHOENIX-ACES-AGSS-COND-2011/train/') \n",
    "#phoenix_wave_path = home+'/'+'/projects/rrg-kyi/group_writable/spectra/grids/phoenix.astro.physik.uni-goettingen.de/v2.0/HiResFITS/PHOENIX-ACES-AGSS-COND-2011/'\n",
    "#ambre_grid_path = os.path.join(home, 'projects/rrg-kyi/group_writable/spectra/grids/AMBRE/train/')\n",
    "obs_wave_filepath = os.path.join(home, 'projects/rrg-kyi/group_writable/spectra/UVES_4835-5395.npy')\n",
    "wave_grid_obs = np.load(obs_wave_filepath)\n",
    "\n",
    "#print(wave_grid_obs)\n",
    "#print(len(wave_grid_obs))\n",
    "#wave_grid_obs = np.linspace(4835,5395,num=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ferre_3500_6000_100'\n",
    "save_path = scratch + '/' + file_name\n",
    "\n",
    "with open(save_path, 'rb') as ferre1_file:\n",
    " \n",
    "    # Step 3\n",
    "    ferre_1_re = pickle.load(ferre1_file)\n",
    " \n",
    "    # After config_dictionary is read from file\n",
    "    print(ferre_1_re.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processor_ferre(wav_ferre,flux_ferre):\n",
    "\n",
    "\n",
    "# Trim the wavelength and flux arrays according to observed wave grid\n",
    "    extension = 10  # Angstroms\n",
    "    wave_min_request = wave_grid_obs[0] - extension\n",
    "    wave_max_request = wave_grid_obs[-1] + extension\n",
    "    #wave_indices_intrigoss = (wav_intrigoss > wave_min_request) & (wav_intrigoss < wave_max_request)\n",
    "    wave_indices_ferre = (wav_ferre > wave_min_request) & (wav_ferre < wave_max_request)\n",
    "    #wave_indices_ambre = (wav_ambre > wave_min_request) & (wav_ambre < wave_max_request)\n",
    "    #wav_intrigoss = wav_intrigoss[wave_indices_intrigoss]\n",
    "    wav_ferre = wav_ferre[wave_indices_ferre]\n",
    "    #wav_ambre = wav_ambre[wave_indices_ambre]\n",
    "    #flux_intrigoss = flux_intrigoss[wave_indices_intrigoss]\n",
    "    flux_ferre = flux_ferre[wave_indices_ferre]\n",
    "    #flux_ambre = flux_ambre[wave_indices_ambre]\n",
    "\n",
    "# Degrade resolution\n",
    "    #err_intrigoss = np.zeros(len(flux_intrigoss))\n",
    "    err_ferre = np.zeros(len(flux_ferre))\n",
    "    #err_ambre = np.zeros(len(flux_ambre))\n",
    "    #_, flux_intrigoss, _ = convolve_spectrum(wav_intrigoss, flux_intrigoss, err_intrigoss, to_resolution=47000)\n",
    "    _, flux_ferre, _ = convolve_spectrum(wav_ferre, flux_ferre, err_ferre, to_resolution=47000)\n",
    "    #_, flux_ambre, _ = convolve_spectrum(wav_ambre, flux_ambre, err_ambre, to_resolution=47000)\n",
    "\n",
    "\n",
    "    \n",
    "# Rebin to UVES wave grid\n",
    "    #flux_intrigoss = rebin(wave_grid_obs, wav_intrigoss, flux_intrigoss)\n",
    "    flux_ferre = rebin(wave_grid_obs, wav_ferre, flux_ferre)\n",
    "    #flux_ambre = rebin(wave_grid_obs, wav_ambre, flux_ambre)\n",
    "\n",
    "# Continuum normalize the spectra\n",
    "    #flux_intrigoss, _ = continuum_normalize(flux_intrigoss, LINE_REGIONS, wave_grid_obs, SEGMENTS_STEP)\n",
    "    flux_ferre, _ = continuum_normalize(flux_ferre, LINE_REGIONS, wave_grid_obs, SEGMENTS_STEP)\n",
    "    #flux_ambre, _ = continuum_normalize(flux_ambre, LINE_REGIONS, wave_grid_obs, SEGMENTS_STEP)\n",
    "\n",
    "    #print('DONE')\n",
    "    \n",
    "    return flux_ferre\n",
    "\n",
    "# Mask telluric lines\n",
    "#flux_intrigoss = mask_tellurics('telluric_lines.txt', flux_intrigoss, wave_grid_obs\n",
    "#flux_phoenix = mask_tellurics('telluric_lines.txt', flux_phoenix, wave_grid_obs)\n",
    "#flux_ambre = mask_tellurics('telluric_lines.txt', flux_ambre, wave_grid_obs)\n",
    "\n",
    "\n",
    "# In[16]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pre_processor_ferre_parallel(wav, fluxes):\n",
    "\n",
    "    @contextmanager\n",
    "    def poolcontext(*args, **kwargs):\n",
    "            pool = multiprocessing.Pool(*args, **kwargs)\n",
    "            yield pool\n",
    "            pool.terminate()\n",
    "\n",
    "    num_spectra = np.shape(fluxes)[0]\n",
    "    num_cpu = multiprocessing.cpu_count()\n",
    "    pool_size = num_cpu if num_spectra >= num_cpu else num_spectra\n",
    "    print('[INFO] Pool size: {}'.format(pool_size))\n",
    "    \n",
    "    pool_arg_list = [(wav, fluxes[i])\n",
    "                         for i in range(num_spectra)]\n",
    "    with poolcontext(processes=pool_size) as pool:\n",
    "            results = pool.starmap(pre_processor_ferre, pool_arg_list)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferre_teff = []\n",
    "ferre_logg = []\n",
    "ferre_feh = []\n",
    "\n",
    "fluxes_ferre = []\n",
    "\n",
    "f_batch_size = 16\n",
    "\n",
    "f_count_tot = 0\n",
    "f_ferre_fin = [] #final array to hold all fluxes\n",
    "\n",
    "for i in ferre_1_re:\n",
    "    feh = i[1]\n",
    "    teff = i[2]\n",
    "    logg = i[3]\n",
    "    \n",
    "    ferre_feh.append(feh)\n",
    "    ferre_teff.append(teff)\n",
    "    ferre_logg.append(logg)\n",
    "    \n",
    "    ferre_flux_sub = []\n",
    "    ferre_wav_sub = []\n",
    "    \n",
    "    for j in i[4:]:\n",
    "        ferre_wav_sub.append(j[0])\n",
    "        \n",
    "        ferre_flux_sub.append(j[1])\n",
    "     \n",
    "    #print(len(ferre_wav_sub))\n",
    "    ferre_wav_sub = np.asarray(ferre_wav_sub)\n",
    "    ferre_flux_sub = np.asarray(ferre_flux_sub)\n",
    "    \n",
    "    fluxes_ferre.append(ferre_flux_sub)\n",
    "    \n",
    "    f_count = len(fluxes_ferre)\n",
    "    \n",
    "    if f_count >= f_batch_size:\n",
    "        t_f = []\n",
    "        t_f=pre_processor_ferre_parallel(ferre_wav_sub,fluxes_ferre)\n",
    "        for i in t_f:\n",
    "            f_ferre_fin.append(i)\n",
    "        f_count_tot+=f_count\n",
    "        f_count = 0\n",
    "        fluxes_ferre = []\n",
    "    \n",
    "print(f_count_tot)\n",
    "print(len(f_ferre_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ferre_flux_p1'\n",
    "\n",
    "save_path = scratch + '/' + file_name\n",
    "\n",
    "with open(save_path, 'wb') as ferre1_file:\n",
    " \n",
    "  # Step 3\n",
    "  pickle.dump(f_ferre_fin, ferre1_file)\n",
    "\n",
    "print(\"First 432 saved!!\")    \n",
    "    #ferre_flux.append(pre_processor_ferre(ferre_wav_sub,ferre_flux_sub))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ferre_5750_8000_100'\n",
    "save_path = scratch + '/' + file_name\n",
    "\n",
    "with open(save_path, 'rb') as ferre2_file:\n",
    " \n",
    "    # Step 3\n",
    "    ferre_2_re = pickle.load(ferre2_file)\n",
    " \n",
    "    # After config_dictionary is read from file\n",
    "    print(ferre_2_re.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_batch_size = 30\n",
    "\n",
    "f_count_tot = 0\n",
    "#f_ferre_fin = [] #final array to hold all fluxes\n",
    "\n",
    "for i in ferre_2_re:\n",
    "    feh = i[1]\n",
    "    teff = i[2]\n",
    "    logg = i[3]\n",
    "    \n",
    "    ferre_feh.append(feh)\n",
    "    ferre_teff.append(teff)\n",
    "    ferre_logg.append(logg)\n",
    "    \n",
    "    ferre_flux_sub = []\n",
    "    ferre_wav_sub = []\n",
    "    \n",
    "    for j in i[4:]:\n",
    "        ferre_wav_sub.append(j[0])\n",
    "        \n",
    "        ferre_flux_sub.append(j[1])\n",
    "     \n",
    "    #print(len(ferre_wav_sub))\n",
    "    ferre_wav_sub = np.asarray(ferre_wav_sub)\n",
    "    ferre_flux_sub = np.asarray(ferre_flux_sub)\n",
    "    \n",
    "    fluxes_ferre.append(ferre_flux_sub)\n",
    "    \n",
    "    f_count = len(fluxes_ferre)\n",
    "    \n",
    "    if f_count >= f_batch_size:\n",
    "        t_f = []\n",
    "        t_f=pre_processor_ferre_parallel(ferre_wav_sub,fluxes_ferre)\n",
    "        for i in t_f:\n",
    "            f_ferre_fin.append(i)\n",
    "        f_count_tot+=f_count\n",
    "        f_count = 0\n",
    "        fluxes_ferre = []\n",
    "    \n",
    "print(f_count_tot)\n",
    "print(f_ferre_fin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ferre_flux_p2'\n",
    "\n",
    "save_path = scratch + '/' + file_name\n",
    "\n",
    "with open(save_path, 'wb') as ferre2_file:\n",
    " \n",
    "  # Step 3\n",
    "  pickle.dump(f_ferre_fin, ferre2_file)\n",
    "\n",
    "print(\"First 432+360 saved!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ferre = np.asarray(f_ferre_fin)\n",
    "Y_ferre = np.column_stack([ferre_teff, ferre_logg, ferre_feh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ferre.shape)\n",
    "print(Y_ferre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(X_ferre[0])\n",
    "print(X_ferre[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teff = ferre_teff\n",
    "feh = ferre_feh\n",
    "logg = ferre_logg\n",
    "a_M = []\n",
    "#for i in range(len(feh)):\n",
    "#    a_M.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "home = os.getenv('HOME')\n",
    "\n",
    "filename = 'ferre_test_100K.h5'\n",
    "preprocessed_spectra_path = os.path.join(home, 'projects/rrg-kyi/group_writable/spectra/preprocessed/')\n",
    "save_path = preprocessed_spectra_path+filename\n",
    "\n",
    "with h5py.File(save_path, 'w') as f:  \n",
    "    f.create_dataset('spectra_starnet_norm', data=np.asarray(X_ferre))\n",
    "    f.create_dataset('teff',data = np.asarray(teff))\n",
    "    f.create_dataset('logg',data = np.asarray(logg))\n",
    "    f.create_dataset('M_H',data = np.asarray(feh))\n",
    "    #f.create_dataset('a_M',data = np.asarray(a_M))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
